
# 用 GRPO 让小模型理解时间范围与粒度 —— 一个真实微调案例

在我们的真实业务场景中，经常需要将用户的自然语言指令转化为结构化的参数，比如：

> “today=20300720 业务需求：比较去年和今年的每日及每周生产量”

看似简单的一句话，背后需要 AI 理解时间上下文，并生成结构化输出，比如：

```json
{"from":"2029-07-20", "to":"2030-07-20", "granularity":["Day","Week"]}
```

我们测试了多个主流大模型（如 ChatGPT 和 DeepSeek），它们基本都能稳定给出符合预期的结构化 JSON 输出。但我们的问题是：

> **是否可以通过微调，让小模型（如 7B 或 8B 规模）也具备同样的能力？**

这正是本次微调实验的出发点。

---

## 微调的动机：给小模型一双“时间感知”的眼睛

我们相信，真正可用的 AI 工具，应该具备低门槛、可部署、不依赖闭源模型的特点。而这意味着小模型的能力提升至关重要。

在这个时间范围提取与粒度识别的任务中，我们希望验证：
- 小模型是否能通过微调获得时间感知能力？
- 哪种微调方法对这类“结构清晰、逻辑明确”的任务最有效？

---

## 为什么选择 GRPO？

相比传统的 SFT 或 RLHF，GRPO（Guided Reinforcement with Prompt Optimization）对样本数量要求较低，更适合工程型任务。

特别是在本任务中，答案本身的形式是非常明确的（一个 JSON 对象），这意味着：

- 标准答案可评估、可衡量
- 训练目标不依赖唯一解
- 可用规则定义多维度 reward function

所以我们选择 GRPO，作为本次任务的核心训练方法。

---

## 数据构建与结构设计

GRPO 决定了我们使用的训练集结构非常简单：

- 每一条样本都包含 `prompt` 与对应的 `completion`
- prompt 是自然语言上下文（包含 today=xxxxxx 以及业务描述）
- completion 是模型应生成的结构化 JSON 输出

例如：

- prompt: today=20300301 业务需求：查看2029年第一季度到2030年第二季度的财务表现，按月季展示  
  completion: {"to":"2030-06-30","from":"2029-01-01","granularity":["Month","Quarter"]}

- prompt: today=20300720 业务需求：比较去年和今年的每日及每周生产量  
  completion: {"to":"2030-07-20","from":"2029-07-20","granularity":["Day","Week"]}

整个数据集共计 1103 条样本。其中一小部分来自真实客户提问，其余样本通过高质量大模型生成与增强。

---

## Reward Function：定义“什么是好答案”

GRPO 的强大之处不仅在于训练结构简单，更在于它可以“灵活定义什么是好”。

我们针对本任务定义了多个维度的奖励函数：

- JSON结构是否完整？是否能解析？
- 日期字段是否格式正确？是否逻辑一致？是否合理覆盖所需时间段？
- 时间粒度是否包含用户期望的周期维度？是否完全匹配？
- today 与生成的时间是否逻辑一致？时间计算有没有出错？

每个维度都有对应的打分机制。最终，奖励是各项得分的加权组合。  
这保证了模型并非追求某一个“正确格式”，而是真正理解时间表达背后的逻辑。

---

## 训练环境与时长

我们在本地服务器使用一张 4090 GPU（24GB）完成训练，运行环境基于 CUDA，使用 DeepExtension 全流程完成。

我们分别对 1.5B、7B 和 8B 模型做了微调，每个模型的训练时长均在一小时以内。

---

## 训练效果：Loss 曲线与 Reward 反馈

训练过程中的 Loss 曲线与 Reward 曲线如下：

- ![训练 Loss 曲线](curve.png)
- ![Reward 得分曲线](reward.png)

可以明显看到，模型在训练过程中迅速掌握了结构与逻辑规律，reward 提升稳定。

---

## 最终效果：模型评估对比

我们使用 DeepExtension 的 Model Assessment 功能，对多个模型组合进行了比较评估。这里展示两个关键组合的结果：

1. **原始 Qwen2.5-7B vs 微调后的 Qwen2.5-1.5B**  
   → 胜出：23，持平：68，失败：1012

2. **微调后的 Qwen2.5-7B vs 微调后的 Qwen2.5-1.5B**  
   → 胜出：1079，持平：23，失败：1

结果非常清晰：微调后的 7B 模型在绝大多数场景中表现稳定、输出准确，完全可用于生产部署。

因此我们决定将 Qwen2.5-7B-fine-tuned 模型作为该任务的主力使用模型。

---

## 小结

这个案例告诉我们：

- 小模型不是不行，而是需要**聪明地训练**
- 像时间范围这种“规则明确”的任务，非常适合使用 GRPO 微调
- 有效的奖励函数设计，是训练成功的关键
- 借助 DeepExtension，可以用最低的成本跑通整个微调闭环

> 如果你也在为某个垂直任务寻找一条“轻量级路径”，或许这就是答案。
