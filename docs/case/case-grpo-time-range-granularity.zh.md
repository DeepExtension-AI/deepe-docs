
# 基于 GRPO 的小模型微调实践：提升时间范围与颗粒度理解能力

在实际业务场景中，常需将用户的自然语言指令转化为结构化参数。例如：

> “today=20300720 业务需求：比较去年和今年的每日及每周生产量”

表面上看似简单的语句，其背后要求 AI 能够精准理解时间上下文，并生成对应的结构化输出，如：

```json
{"from":"2029-07-20", "to":"2030-07-20", "granularity":["Day","Week"]}
```

在对多个主流大模型（如 ChatGPT 和 DeepSeek）的测试中，本文发现这些模型基本能够稳定地输出符合预期的结构化 JSON 结果。然而，问题的核心在于：

> **是否可以通过微调，让小模型（如 7B 或 8B 规模）也具备同样的能力？**

这正是本次微调实验的出发点与研究方向。

---

## 微调的动机：赋予小模型“时间感知”能力

我们相信，真正可用的 AI 工具，应该具备低门槛、可部署、不依赖闭源模型的特点。而这意味着小模型的能力提升至关重要。

本文主张，真正实用的 AI 工具应具备低门槛、易部署、不依赖封闭式预训练模型的特性，而小模型的能力拓展在此背景下显得尤为重要。针对时间范围提取与粒度识别任务，本实验旨在验证以下两个关键问题：

- 小模型是否能够通过有效的微调过程获得精准的时间感知能力？
- 在处理这类“结构清晰、逻辑明确”的任务时，何种微调方法能够达到最佳效果？

---

## GRPO 方法选择依据

相比传统的监督微调（SFT）或基于人类反馈的强化学习（RLHF），GRPO（Guided Reinforcement with Prompt Optimization）在样本需求方面具有显著优势，尤其在样本稀缺时，其优势更为显著，因此更适合工程型任务。

在本任务中，答案形式明确为 JSON 对象，具备以下特性：

- 标准答案可评估且可衡量
- 训练目标不依赖唯一解
- 可基于规则定义多维度 reward function

综合以上考量，本实验选定 GRPO 作为核心训练方法。

---

## 数据构建与结构设计

在采用 GRPO 方法时，训练集的结构设计极为简洁。每条训练样本均包含以下两个部分：

- 每一条样本都包含 `prompt` 与对应的 `completion`
- prompt 是自然语言上下文，包含特定的时间标记（today=xxxxxx）及业务描述
- completion 是模型需要生成的结构化 JSON 输出

例如：

- prompt: today=20300301 业务需求：查看2029年第一季度到2030年第二季度的财务表现，按月季展示  
  completion: {"to":"2030-06-30","from":"2029-01-01","granularity":["Month","Quarter"]}

- prompt: today=20300720 业务需求：比较去年和今年的每日及每周生产量  
  completion: {"to":"2030-07-20","from":"2029-07-20","granularity":["Day","Week"]}

本次实验所用的数据集包含 1103 条样本。其中部分样本源自真实客户提问，其余样本则通过高质量大模型生成与增强方法创造。

---

## Reward Function：优质答案的多维度定义

GRPO 的优势不仅体现在训练结构的简洁性上，更在于其能够灵活地定义优质答案的标准。
针对本任务，本文设计了多维度的奖励函数，以全面评估模型输出：

- JSON 结构的完整性与可解析性。
- 日期字段的格式正确性、逻辑一致性和对所需时间段的合理覆盖。
- 时间粒度是否包含用户期望的周期维度，以及是否完全匹配。
- today 标记与生成时间的逻辑一致性，是否存在时间计算错误。

每个维度均配备相应的打分机制，最终奖励为各项得分的加权组合。这种设计确保模型不仅追求单一的“正确格式”，而是深刻理解时间表达背后的逻辑关系，从而生成真正符合用户需求的优质答案。

---

## 训练环境与时长

在配备单张 NVIDIA GeForce RTX 4090 GPU（24GB 显存）的本地服务器上完成训练，运行环境基于 CUDA，通过 DeepExtension 框架实现全流程训练。

本次实验对不同规模的模型进行了微调，涵盖 1.5B、7B 和 8B 模型，每个模型的训练时长均控制在一小时以内。

---

## 训练效果：Loss 曲线与 Reward 反馈

训练过程中的 Loss 曲线与 Reward 曲线如下：

- ![训练 Loss 曲线](curve.png)
- ![Reward 得分曲线](reward.png)

可以明显观察到，模型在训练过程中迅速掌握了结构与逻辑规律，Reward 值呈稳定上升趋势，表明模型的性能在不断优化。

---

## 最终效果：模型评估对比

利用 DeepExtension 的 Model Assessment 功能，对多个模型组合进行了详尽的比较评估。以下为两个关键组合的评估结果：

1. **原始 Qwen2.5-7B 模型与微调后的 Qwen2.5-1.5B 模型对比**  
   → 胜出：23，持平：68，失败：1012

2. **微调后的 Qwen2.5-7B 模型与微调后的 Qwen2.5-1.5B 模型对比**  
   → 胜出：1079，持平：23，失败：1

结果表明，微调后的 7B 模型在绝大多数场景中表现出色，展现出优异的稳定性和准确性，已完全符合生产环境的部署要求。基于此，本文决定将 
Qwen2.5-7B-fine-tuned 模型作为该任务的主力使用模型。

---

## 小结

本案例表明：

- 小型模型并非不可用，关键在于采用**正确的训练策略**。
- 对于时间范围提取这类“规则明确”的任务，GRPO 微调方法展现出了显著优势。
- 设计有效的奖励函数是实现成功训练的核心要素。
- 通过 DeepExtension 工具，能够以极低的成本高效完成整个微调流程。

> 对于那些致力于为特定垂直任务寻找“轻量级解决方案”的研究者或实践者而言，本研究提供了一种可行且高效的路径选择。
