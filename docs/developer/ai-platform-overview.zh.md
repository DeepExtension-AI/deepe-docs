# 支持的 AI 平台及其意义

DeepExtension 致力于弥合企业用户与大语言模型（LLM）训练复杂性之间的鸿沟。实现这一使命的关键之一是支持在多种硬件和操作系统上兼顾**性能**、**易用性**与**开发者友好度**的 AI 平台。

本页面将介绍当前支持的平台、背后的技术选择逻辑，以及这些平台如何影响 DeepExtension 的用户体验。

---

## 为什么平台选择至关重要？

AI 平台的选择直接影响：

- **对主流 ML 框架的兼容性**（如 PyTorch、TensorFlow）
- **训练与推理的性能表现**
- **安装与部署的简便性**
- **硬件成本与可获取性**

对于非 AI 专业人员或资源有限的团队，复杂的配置过程可能成为门槛。  
DeepExtension 的目标是**降低使用门槛，同时不牺牲性能**，因此平台支持是一个战略性决策。

---

## CUDA 平台：LLM 训练的行业标准

从 DeepExtension 的早期阶段开始，我们便采用 **CUDA** 作为主要的训练与推理后端。CUDA 支持的 NVIDIA GPU 仍是以下场景的**事实标准**：

- 完全兼容 PyTorch 与 TensorFlow  
- 针对 LLM 架构与大规模并行处理进行了优化  
- 成熟的生态系统（工具链、社区、研究支持）

这使得 CUDA 成为严肃训练任务中最**可靠且高性能**的选择。  
DeepExtension 的训练模块（如 GRPO 和 SFT）已全面优化以适配 CUDA 环境。

> 对于需要大规模微调开源基础模型（如 Qwen、LLaMA、DeepSeek）的企业用户，强烈推荐使用 CUDA。

---

## Apple Silicon 上的 MLX：轻量与易用兼具

尽管 CUDA 性能强劲，但并非所有用户都易于获取 —— 尤其是个体研究者或缺乏 NVIDIA 硬件的小团队。

Apple 的 M 系列芯片（M1–M4）带来了新的机遇。凭借其**统一内存架构（UMA）**与出色的本地 AI 能力，它们具备：

- 紧凑却强大的开发环境  
- 无需外接 GPU —— AI 模块内建于芯片  
- 安静节能，适合日常开发使用

我们曾尝试在 macOS 上测试 PyTorch 的 Metal 后端（MPS），但发现其性能不稳定、兼容性较差。

因此我们选择集成 **MLX** —— Apple 专为其芯片架构开发的新一代机器学习框架。MLX 的优势包括：

- 实际场景中远胜于 MPS 的性能表现  
- 更简洁的配置与内存管理  
- 小规模训练与实验的高效选择

DeepExtension 已内置 **MLX 演示训练任务**，方便 Mac 用户即刻体验微调流程。

---

## 当前支持的平台

| 平台         | 后端         | 支持操作系统           | 典型应用场景                         |
|--------------|--------------|------------------------|--------------------------------------|
| **CUDA**     | PyTorch / TensorFlow | Linux，Windows（通过 WSL） | 全流程模型训练，生产部署             |
| **MLX**      | MLX          | macOS（M1–M4）         | 本地开发，小规模训练                 |

> 其他平台暂不支持，未来将根据用户需求优先考虑。

---

## 平台支持未来规划

我们理解全球用户在硬件与生态偏好方面存在多样性。

尽管 CUDA 与 MLX 已覆盖大多数当前使用场景，我们仍在积极收集反馈，以评估以下平台的支持优先级：

- Windows（用于 CPU 推理与 UI 体验测试）
- AMD ROCm 平台（更开放的 GPU 生态）
- ONNX Runtime 或 TensorRT（专注推理优化的部署方案）

如果您有特定平台需求或环境限制，请通过 [支持页面](../about/support.md) 联系我们。您的反馈将直接影响我们的产品路线图。

---

*DeepExtension 旨在让 LLM 训练更易获取 —— 无论您是在数据中心、MacBook 还是创业阶段的预算环境下使用。*
