# DeepPrompt

**DeepPrompt** 是 DeepExtension 中强大的提示词实验工具，允许用户设计、测试并比较在不同模型状态和来源下的结构化提示词。它专为 **业务逻辑对齐**、**提示词评估** 和 **模型行为可复现性** 设计，无需编写代码即可实现。

---

## DeepPrompt 有何独特之处

与传统的 prompt playground 不同，DeepPrompt 提供与 **您模型和嵌入式企业文档的紧密集成**，从而实现更深层次的控制和更真实的测试环境。

主要特性包括：

- **使用已训练模型进行推理**：您可以直接对 **训练生成的适配器（如 PEFT 检查点）** 进行推理，甚至无需先将其合并至基础模型。在模型选择界面，系统会列出所有 **训练模型** 和 **第三方模型**，并明确标注模型类型。

- **嵌入文档支持**：您可以在提示词中直接引用企业文档或已嵌入的知识库，这对 RAG（检索增强生成）类用例尤为重要。

- **System Prompt 控制**：强烈建议填写 **System Prompt**，这将引导模型行为并提升推理质量。如果留空，系统将自动注入如 *“I am an AI assistant.”* 的默认信息。

- **模型并排比较**：点击 **“Add a model”** 可轻松比较两个模型的推理结果。通过 **“Copy parameters”** 复制上方模型的所有提示设置，实现快速对比。

- **真实生产模拟**：可在生产模拟流中测试部分或中间训练成果，从而在完整部署或模型合并前 **提前评估提示词效果**。

---

### 注意事项

- 对于 **训练模型**，其对应的 **保存模型（Saved Model）** 在合并后，理论上应生成相同的推理结果。
- **强烈建议使用已部署的保存模型** 作为生产环境中的推理模型，其推理速度更快、稳定性更高。
- 针对训练模型的推理主要用于 **验证和实验**，不适用于高负载或低延迟生产需求。
- **基础模型** 不可直接用于 DeepPrompt 推理，如需使用，请先部署以获取更快性能。

---

## 典型用途

- 跨模型版本评估提示词鲁棒性  
- 多配置下推理行为比较测试  
- 使用真实企业数据验证业务规则  
- 测试文档增强型问答能力  
- 为内部工具或 API 调整提示词

---

## 下一步建议

- 选择模型并填写输入提示词  
- 按需嵌入参考文档或系统提示  
- 使用对比模式并排评估推理输出  
- 与团队共享提示词测试记录

---

*DeepExtension — 让提示词更精准，模型管理更企业级*