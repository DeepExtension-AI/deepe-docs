
# 模型评估

**模型评估**模块是 DeepExtension 提供的一套强大的批量评估框架，用于通过真实世界的数据集评估和对比语言模型的输出表现。该模块旨在帮助用户在大规模下从定量与定性两个维度评估模型行为、输出质量与对齐程度。

---

## 评估模式

DeepExtension 支持四种灵活的评估模式：

- **单模型候选项**：从一个模型生成答案，不进行判断，仅输出内容。
- **双模型候选项**：从两个模型生成答案，并进行并排对比。
- **1 候选 + 1 评审**：由一个评审模型根据预设标准评估候选模型的回答。
- **2 候选 + 1 评审**：由评审模型对比两个候选答案，并给出更优选项及评分或解释。

---

## 创建新的评估任务

1. 在“模型评估”页面点击 **“新建评估”**
2. 选择评估模式、评估用的数据集、样本数量（默认使用全部样本）
3. 选择 **模型 A**，如适用还可选择 **模型 B** 和 **评审模型**。  
   模型选择范围与 [Deep Prompt](deep-prompt.zh.md) 一致，支持：

    - 第三方模型（如 OpenAI、Anthropic、ModelScope 等）
    - 已训练模型
    - 已部署模型

4. 定义提示词（Prompt）：

    - **候选模型系统提示词**：用于推理的系统提示（参见 [DeepPrompt](deep-prompt.zh.md)）。如为空，将自动注入默认系统消息，如 *“我是一个 AI 助手。”*
    - **候选模型用户提示词**：必须包含动态占位符如 `{{column_name}}`，以从数据集中提取值，例如 `请简要回答问题：{{question}}`
    - **评审模型系统提示词**：告知评审模型采用何种评估标准，可包含 `{{CandidateSystemPrompt}}`
    - **评审模型用户提示词**：使用 `{{ResponseA}}`、`{{ResponseB}}`、`{{ref_answer}}` 等占位符，例如：`根据参考答案 {{ref_answer}}，比较答案 A：{{ResponseA}} 与答案 B：{{ResponseB}}`

---

## 执行前预览

在启动评估前，可使用最多 5 条样本进行预览，验证提示词与模型输出是否正确：

- 点击 **“预览”**
- 查看模型输出与提示词注入结果
- 根据需要进行修改

确认无误后，点击 **“提交评估”**，评估过程将以 **批量模式** 执行。

---

## 查看评估结果

在“模型评估”主页面：

- 点击 **“查看”** 可查看已完成的评估任务
- 包含三个标签页：
  - **评估概览**：显示所有配置详情
  - **日志**：展示评估过程中的系统日志
  - **结果**：以表格形式展示最终结果

---

## 下载评估结果

在评估任务完成后，点击 **“下载”** 可将结果导出为 `.csv` 文件。

---

## 复制现有评估

若想基于现有评估快速调整配置并再次运行：

- 打开任一已完成的评估任务，进入 **“评估概览”** 标签页
- 点击 **“复制”**
- 所有配置项将自动填充，可快速修改并重新运行评估

---

*DeepExtension — 为企业打造的可扩展、灵活、可解释的大模型评估系统*
